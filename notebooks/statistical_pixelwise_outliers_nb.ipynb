{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "import numpy as np\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from skimage.measure import label\n",
    "from skimage.segmentation import find_boundaries\n",
    "from torch.utils.data import DataLoader\n",
    "# from ipywidgets import interact\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "import folium\n",
    "import matplotlib.colors as mcolors\n",
    "import unfoldNd\n",
    "import itertools\n",
    "\n",
    "\n",
    "from model.mlp import MLP\n",
    "from tools.train_double_logistic import double_logistic_function\n",
    "from utils.helpers import get_doy, check_missing_timestamps\n",
    "\n",
    "# from data.patchwise_dataset import PatchwiseDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"/Volumes/Macintosh HD/Users/davidbruggemann/OneDrive/DIMPEO/data/tmp9_train.h5\"\n",
    "\n",
    "# ds = PatchwiseDataset(\n",
    "#         file_path,\n",
    "#         pixelwise=False,\n",
    "#         annual=False,\n",
    "#         spatiotemporal_features=[\"s2_ndvi\", \"s2_B04\", \"s2_B03\", \"s2_B02\"],\n",
    "#         spatial_features=[\"valid_mask\", \"drought_mask\"],\n",
    "#     )\n",
    "\n",
    "# torch.manual_seed(1)\n",
    "# loader = DataLoader(ds, batch_size=1, drop_last=True, shuffle=True)\n",
    "# loader_iter = iter(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTLIER_FACTOR = 1.5\n",
    "OUTLIER_FACTOR_NN = 1.5\n",
    "MAX_MISSINGNESS = 0.95\n",
    "COUNT_THRESHOLD = 100\n",
    "WINDOW_SIZE = 3\n",
    "T_WINDOW_SIZE = 5\n",
    "NUM_YEARS = 7\n",
    "NUM_DATAPOINTS_PER_YEAR = 73\n",
    "H = 128\n",
    "W = 128\n",
    "START_YEAR = 2017\n",
    "END_YEAR = 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.rcParams[\"figure.figsize\"] = (16,8)\n",
    "\n",
    "# def browse_images(rgb, ndvi, l_bound, h_bound):\n",
    "#     N = rgb.shape[0]\n",
    "#     ndvi_vmin = ndvi[~torch.isnan(ndvi)].min()\n",
    "#     ndvi_vmax = ndvi[~torch.isnan(ndvi)].max()\n",
    "#     fig, ax = plt.subplots(1, 2, sharey=True)\n",
    "#     im1 = ax[0].imshow(rgb[0])\n",
    "#     im2 = ax[0].imshow(l_bound[0, :, :], cmap=matplotlib.colors.ListedColormap(['white', 'red']), alpha=1.0 * (l_bound[0, :, :] > 0))\n",
    "#     im3 = ax[0].imshow(h_bound[0, :, :], cmap=matplotlib.colors.ListedColormap(['blue', 'white']), alpha=1.0 * (h_bound[0, :, :] > 0))\n",
    "#     im4 = ax[1].imshow(ndvi[0], vmin=ndvi_vmin, vmax=ndvi_vmax)\n",
    "#     im5 = ax[1].imshow(l_bound[0, :, :], cmap=matplotlib.colors.ListedColormap(['white', 'red']), alpha=1.0 * (l_bound[0, :, :] > 0))\n",
    "#     im6 = ax[1].imshow(h_bound[0, :, :], cmap=matplotlib.colors.ListedColormap(['blue', 'white']), alpha=1.0 * (h_bound[0, :, :] > 0))\n",
    "#     fig.colorbar(im4, ax=ax, fraction=0.046, pad=0.04)\n",
    "#     fig.suptitle(\"red: NDVI too low, blue: NDVI too high\")\n",
    "#     def update(i=0):\n",
    "#         im1.set_data(rgb[i])\n",
    "#         im2.set_data(l_bound[i, :, :])\n",
    "#         im2.set_alpha(1.0 * (l_bound[i, :, :] > 0))\n",
    "#         im3.set_data(h_bound[i, :, :])\n",
    "#         im3.set_alpha(1.0 * (h_bound[i, :, :] > 0))\n",
    "#         im4.set_data(ndvi[i])\n",
    "#         im5.set_data(l_bound[i, :, :])\n",
    "#         im5.set_alpha(1.0 * (l_bound[i, :, :] > 0))\n",
    "#         im6.set_data(h_bound[i, :, :])\n",
    "#         im6.set_alpha(1.0 * (h_bound[i, :, :] > 0))\n",
    "#         fig.canvas.draw_idle()\n",
    "#     interact(update, i=(0, N-1));\n",
    "\n",
    "# def lin_stretch_vec(band):\n",
    "#     v = torch.nanquantile(band, torch.tensor((0.02, 0.98), dtype=band.dtype))\n",
    "#     temp = 255 * (band - v[0]) / (v[1] - v[0])\n",
    "#     temp = torch.clip(temp, 0, 255)\n",
    "#     return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map(minicube):\n",
    "    x_min, x_max, y_min, y_max = minicube.lon[0].values, minicube.lon[-1].values, minicube.lat[-1].values, minicube.lat[0].values\n",
    "    # Create map\n",
    "    interactive_map = folium.Map(\n",
    "        location=((y_min+y_max)/2, (x_min+x_max)/2),\n",
    "        zoom_start=min(int((np.log(1000 / (x_max-x_min)) / np.log(2))), int((np.log(1000 / (y_max-y_min)) / np.log(2)))),\n",
    "        tiles=\"http://mt1.google.com/vt/lyrs=y&z={z}&x={x}&y={y}\",\n",
    "        attr=\"Google\",\n",
    "        prefer_canvas=True,\n",
    "    )\n",
    "    \n",
    "    folium.TileLayer('openstreetmap', show=False).add_to(interactive_map)\n",
    "    \n",
    "    # Create bounding box coordinates to overlay on map\n",
    "    line_segments = [\n",
    "        (y_min, x_min),\n",
    "        (y_min, x_max),\n",
    "        (y_max, x_max),\n",
    "        (y_max, x_min),\n",
    "        (y_min, x_min),\n",
    "    ]\n",
    "\n",
    "    # Add bounding box as an overlay\n",
    "    interactive_map.add_child(\n",
    "        folium.features.PolyLine(locations=line_segments, color=\"black\", opacity=0.8)\n",
    "    )\n",
    "    \n",
    "    return interactive_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder='/data_2/dimpeo/cubes'\n",
    "filepaths = sorted(list(glob.glob(os.path.join(folder, \"*_raw.nc\"))))\n",
    "device = \"cuda\"\n",
    "\n",
    "\n",
    "def cartesian_product(a, b):\n",
    "    return np.array(list(itertools.product(a, b)))\n",
    "\n",
    "\n",
    "encoder = MLP(d_in=8, d_out=8, n_blocks=8, d_block=256, dropout=0, skip_connection=True).to(device)\n",
    "encoder.load_state_dict(torch.load(\"/data_1/scratch_1/dbrueggemann/nn/encoder_noposenc_notwi.pt\"))\n",
    "encoder.eval()\n",
    "\n",
    "# rescale input (stats obtained from full dataset)\n",
    "# MEANS = torch.tensor([\n",
    "#     8.34458,  # lon\n",
    "#     46.32019,  # lat\n",
    "#     1104.5027,  # dem\n",
    "#     43.102547,  # fc\n",
    "#     15.638395,  # fh\n",
    "#     26.418936,  # slope\n",
    "#     -0.0069493465,  # easting\n",
    "#     0.062305786,  # northing\n",
    "#     2.8649843,  # twi\n",
    "#     ]).to(device).unsqueeze(1).unsqueeze(1)\n",
    "# STDS = torch.tensor([\n",
    "#     0.9852665,  # lon\n",
    "#     0.61929077,  # lat\n",
    "#     409.8894,  # dem\n",
    "#     40.840992,  # fc\n",
    "#     30.395235,  # fh\n",
    "#     13.358144,  # slope\n",
    "#     0.69276434,  # easting\n",
    "#     0.7184338,  # northing\n",
    "#     1.861385,  # twi\n",
    "#     ]).to(device).unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "MEANS = {\n",
    "    \"lon\": 8.34458,\n",
    "    \"lat\": 46.32019,\n",
    "    \"dem\": 1104.5027,\n",
    "    \"fc\": 43.102547,\n",
    "    \"fh\": 15.638395,\n",
    "    \"slope\": 26.418936,\n",
    "    \"easting\": -0.0069493465,\n",
    "    \"northing\": 0.062305786,\n",
    "    \"twi\": 2.8649843,\n",
    "    \"rugg\": 8.363732,\n",
    "    \"curv\": 6.1888146e-05,\n",
    "}\n",
    "STDS = {\n",
    "    \"lon\": 0.9852665,\n",
    "    \"lat\": 0.61929077,\n",
    "    \"dem\": 409.8894,\n",
    "    \"fc\": 40.840992,\n",
    "    \"fh\": 30.395235,\n",
    "    \"slope\": 13.358144,\n",
    "    \"easting\": 0.69276434,\n",
    "    \"northing\": 0.7184338,\n",
    "    \"twi\": 1.861385,\n",
    "    \"rugg\": 5.4581103,\n",
    "    \"curv\": 0.0061930786,\n",
    "}\n",
    "features = [\"lon\", \"lat\", \"dem\", \"fc\", \"fh\", \"slope\", \"easting\", \"northing\"]\n",
    "means = torch.tensor([MEANS[f] for f in features], dtype=float, device=device).unsqueeze(1).unsqueeze(1)\n",
    "stds = torch.tensor([STDS[f] for f in features], dtype=float, device=device).unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "T_SCALE = 1.0 / 365.0\n",
    "t_plot = torch.linspace(0, 365, 1000).unsqueeze(0) * T_SCALE\n",
    "YEARS_IN_TRAIN = 6\n",
    "SPLIT_IDX = 73 * YEARS_IN_TRAIN\n",
    "\n",
    "\n",
    "# for path in filepaths:\n",
    "while True:\n",
    "    with torch.no_grad():\n",
    "        rand_ix = np.random.randint(0, len(filepaths))\n",
    "        path = filepaths[rand_ix]\n",
    "\n",
    "        try:\n",
    "            minicube = xr.open_dataset(path, engine=\"h5netcdf\")\n",
    "        except OSError:\n",
    "            continue\n",
    "\n",
    "        missing_dates = check_missing_timestamps(minicube, START_YEAR, END_YEAR)\n",
    "        if missing_dates:\n",
    "            minicube = minicube.reindex(\n",
    "                time=np.sort(np.concatenate([minicube.time.values, missing_dates]))\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            ndvi = minicube.s2_ndvi.where((minicube.s2_mask == 0) & minicube.s2_SCL.isin([1, 2, 4, 5, 6, 7])).values\n",
    "            forest_mask = (minicube.FOREST_MASK.values > 0.8)\n",
    "        except AttributeError:\n",
    "            continue\n",
    "\n",
    "        # convert to torch\n",
    "        ndvi = torch.from_numpy(ndvi).to(device)\n",
    "        doy = torch.from_numpy(get_doy(minicube.time.values)).to(device)\n",
    "        forest_mask = torch.from_numpy(forest_mask).to(device)\n",
    "\n",
    "        # mask out non-forest pixels\n",
    "        ndvi[:, ~forest_mask] = torch.nan\n",
    "\n",
    "        # features for network\n",
    "        longitude = np.array(minicube.lon.values, dtype=np.float32)\n",
    "        latitude = np.array(minicube.lat.values, dtype=np.float32)\n",
    "        lon_lat = cartesian_product(longitude, latitude).reshape(len(longitude), len(latitude), 2)\n",
    "        lon_lat = torch.from_numpy(lon_lat).permute(2, 0, 1).to(device)\n",
    "        dem = torch.from_numpy(minicube.DEM.values).unsqueeze(0).to(device)\n",
    "        fc = torch.from_numpy(minicube.FC.values).unsqueeze(0).to(device)\n",
    "        fh = torch.from_numpy(minicube.FH.values).unsqueeze(0).to(device)\n",
    "        slope = torch.from_numpy(minicube.slope.values).unsqueeze(0).to(device)\n",
    "        easting = torch.from_numpy(minicube.easting.values).unsqueeze(0).to(device)\n",
    "        northing = torch.from_numpy(minicube.northing.values).unsqueeze(0).to(device)\n",
    "        # twi = torch.from_numpy(minicube.twi.values).unsqueeze(0).to(device)\n",
    "\n",
    "        fc[fc == -9999] = torch.nan\n",
    "        inp = torch.cat([lon_lat, dem, fc, fh, slope, easting, northing], axis=0)  # 9 x H x W\n",
    "        inp = (inp - means) / stds\n",
    "        inp = torch.nan_to_num(inp, nan=0.0)\n",
    "\n",
    "        # only forward forest pixels\n",
    "        masked_inp = inp[:, forest_mask]\n",
    "        masked_inp = masked_inp.permute(1, 0)  # B x 9\n",
    "\n",
    "        preds = encoder(masked_inp.float())\n",
    "        paramsl = preds[:, [0, 1, 2, 3, 4, 5]]\n",
    "        paramsu = torch.cat([preds[:, [0, 1, 2, 3]], preds[:, [4, 5]] + nn.functional.softplus(preds[:, [6, 7]])], axis=1)\n",
    "\n",
    "        # find anomalies\n",
    "        doy_test = doy[SPLIT_IDX:]\n",
    "        ndvi_lower_pred = double_logistic_function(doy_test.unsqueeze(0).repeat(paramsl.shape[0], 1), paramsl).permute(1, 0)\n",
    "        ndvi_upper_pred = double_logistic_function(doy_test.unsqueeze(0).repeat(paramsu.shape[0], 1), paramsu).permute(1, 0)\n",
    "\n",
    "        iqr = torch.abs(ndvi_upper_pred - ndvi_lower_pred)\n",
    "\n",
    "        l_thresh = (ndvi_lower_pred - OUTLIER_FACTOR_NN * iqr)  # 73 x B\n",
    "        h_thresh = (ndvi_upper_pred + OUTLIER_FACTOR_NN * iqr)\n",
    "\n",
    "        l_anom_nn = torch.full((NUM_DATAPOINTS_PER_YEAR, H, W), torch.nan, device=device)\n",
    "        h_anom_nn = torch.full((NUM_DATAPOINTS_PER_YEAR, H, W), torch.nan, device=device)\n",
    "        ndvi_test = ndvi[SPLIT_IDX:, forest_mask]\n",
    "        ndvi_nan_mask = torch.isnan(ndvi_test)\n",
    "        l_anoms = (ndvi_test < l_thresh).float()\n",
    "        l_anoms[ndvi_nan_mask] = torch.nan\n",
    "        l_anom_nn[:, forest_mask] = l_anoms\n",
    "        h_anoms = (ndvi_test > h_thresh).float()\n",
    "        h_anoms[ndvi_nan_mask] = torch.nan\n",
    "        h_anom_nn[:, forest_mask] = h_anoms\n",
    "\n",
    "        # # TODO: do post-processing\n",
    "        # l_anom_nn = torch.full((NUM_DATAPOINTS_PER_YEAR, H, W), torch.nan, device=device)\n",
    "        # h_anom_nn = torch.full((NUM_DATAPOINTS_PER_YEAR, H, W), torch.nan, device=device)\n",
    "        # diff = (l_thresh - ndvi[SPLIT_IDX:, forest_mask]) / iqr\n",
    "        # diff = torch.clamp(diff, min=0)\n",
    "        # # diff[torch.isnan(diff)] = 0\n",
    "        # l_anom_nn[:, forest_mask] = diff.float()\n",
    "\n",
    "        # diff = (ndvi[SPLIT_IDX:, forest_mask] - h_thresh) / iqr\n",
    "        # diff = torch.clamp(diff, min=0)\n",
    "        # # diff[ndvi[SPLIT_IDX:, forest_mask] <= h_thresh] = 0\n",
    "        # # diff[torch.isnan(diff)] = 0\n",
    "        # h_anom_nn[:, forest_mask] = diff.float()\n",
    "\n",
    "        # l_anom_nn_sp = torch.tensor_split(l_anom_nn, 12, dim=0)\n",
    "        # h_anom_nn_sp = torch.tensor_split(h_anom_nn, 12, dim=0)\n",
    "        \n",
    "        # l_anom_nn[:, forest_mask] = ndvi[SPLIT_IDX:, forest_mask] < l_thresh\n",
    "        \n",
    "\n",
    "        # group by day of the year\n",
    "        # ndvi = (\n",
    "        #     ndvi.view(NUM_YEARS, NUM_DATAPOINTS_PER_YEAR, H, W)\n",
    "        #     .permute(1, 0, 2, 3)\n",
    "        #     .contiguous()\n",
    "        # )\n",
    "        ndvi_grouped = ndvi.view(1, NUM_YEARS, NUM_DATAPOINTS_PER_YEAR, H, W)\n",
    "\n",
    "        # separate fitting set from test\n",
    "        fitset = ndvi_grouped[:, :NUM_YEARS - 1]\n",
    "        testset = ndvi_grouped[:, NUM_YEARS - 1]\n",
    "\n",
    "        padding = int((WINDOW_SIZE - 1) / 2)\n",
    "        t_padding = int((T_WINDOW_SIZE - 1) / 2)\n",
    "        # pad in space\n",
    "        fitset_pad = nn.functional.pad(fitset, (padding, padding, padding, padding, 0, 0), \"constant\", torch.nan)\n",
    "        # pad in time\n",
    "        fitset_pad = torch.cat((fitset_pad[:, :, -t_padding:], fitset_pad, fitset_pad[:, :, :t_padding]), dim=2)\n",
    "\n",
    "        unfold = unfoldNd.UnfoldNd(kernel_size=(T_WINDOW_SIZE, WINDOW_SIZE, WINDOW_SIZE), dilation=1, padding=0, stride=1)\n",
    "        fitset_unfold = unfold(fitset_pad)\n",
    "        \n",
    "        valid_mask_unfold = forest_mask.view(1, 1, 1, H, W).expand(1, fitset_unfold.shape[1], NUM_DATAPOINTS_PER_YEAR, H, W).reshape(1, -1, NUM_DATAPOINTS_PER_YEAR * H * W)\n",
    "        fitset_unfold[~valid_mask_unfold.bool()] = torch.nan\n",
    "\n",
    "        # pixels with too few observations are ignored\n",
    "        invalid_mask = torch.sum(torch.isnan(fitset_unfold), dim=1, keepdim=True) / fitset_unfold.shape[1] > MAX_MISSINGNESS\n",
    "        fitset_unfold[invalid_mask.expand_as(fitset_unfold)] = torch.nan\n",
    "\n",
    "        q1 = torch.nanquantile(fitset_unfold, 0.25, dim=1)\n",
    "        q3 = torch.nanquantile(fitset_unfold, 0.75, dim=1)\n",
    "        iqr = torch.abs(q3 - q1)\n",
    "\n",
    "        l_thresh = (q1 - OUTLIER_FACTOR * iqr).view(NUM_DATAPOINTS_PER_YEAR, H, W)\n",
    "        h_thresh = (q3 + OUTLIER_FACTOR * iqr).view(NUM_DATAPOINTS_PER_YEAR, H, W)\n",
    "\n",
    "        l_anom = testset.squeeze() < l_thresh  # 73 x H x W\n",
    "        h_anom = testset.squeeze() > h_thresh\n",
    "\n",
    "        # l_labels, l_num = label(l_anom, background=0, connectivity=2, return_num=True)\n",
    "        # for c in range(1, l_num + 1):\n",
    "        #     count = np.count_nonzero(l_labels == c)\n",
    "        #     if count < COUNT_THRESHOLD:\n",
    "        #         l_labels[l_labels == c] = 0\n",
    "        # l_labels[l_labels > 0] = 1\n",
    "\n",
    "        # h_labels, h_num = label(h_anom, background=0, connectivity=2, return_num=True)\n",
    "        # for c in range(1, h_num + 1):\n",
    "        #     count = np.count_nonzero(h_labels == c)\n",
    "        #     if count < COUNT_THRESHOLD:\n",
    "        #         h_labels[h_labels == c] = 0\n",
    "        # h_labels[h_labels > 0] = 1\n",
    "\n",
    "        # anom = l_labels | h_labels\n",
    "\n",
    "        # labels, num = label(anom, background=0, connectivity=2, return_num=True)\n",
    "\n",
    "        # print(\"Found {} anomalies.\".format(num))\n",
    "\n",
    "        # ndvi = testset.squeeze(0)\n",
    "\n",
    "        # red = sample[\"spatiotemporal\"][:, -NUM_DATAPOINTS_PER_YEAR:, :, :, 1].squeeze(0)\n",
    "        # green = sample[\"spatiotemporal\"][:, -NUM_DATAPOINTS_PER_YEAR:, :, :, 2].squeeze(0)\n",
    "        # blue = sample[\"spatiotemporal\"][:, -NUM_DATAPOINTS_PER_YEAR:, :, :, 3].squeeze(0)\n",
    "\n",
    "        # # mask out invalid\n",
    "        # red[(valid_mask == 0).view(1, H, W).expand_as(red)] = torch.nan\n",
    "        # green[(valid_mask == 0).view(1, H, W).expand_as(green)] = torch.nan\n",
    "        # blue[(valid_mask == 0).view(1, H, W).expand_as(blue)] = torch.nan\n",
    "        # red_mask = ~torch.isnan(red)\n",
    "        # green_mask = ~torch.isnan(green)\n",
    "        # blue_mask = ~torch.isnan(blue)\n",
    "        # rgb_mask = red_mask & green_mask & blue_mask\n",
    "\n",
    "        # # R: https://www.rdocumentation.org/packages/raster/versions/3.6-26/topics/plotRGB\n",
    "        # red = lin_stretch_vec(red)\n",
    "        # green = lin_stretch_vec(green)\n",
    "        # blue = lin_stretch_vec(blue)\n",
    "        # rgb = torch.stack((red, green, blue), dim=-1)  # T x H x W x 3\n",
    "        # rgb[~rgb_mask.unsqueeze(-1).expand_as(rgb)] = torch.nan\n",
    "        # rgb = torch.nan_to_num(rgb, nan=0).to(torch.uint8)\n",
    "\n",
    "        # rgb = torch.stack((red, green, blue), dim=-1)  # T x H x W x 3\n",
    "        # rgb[~rgb_mask.unsqueeze(-1).expand_as(rgb)] = torch.nan\n",
    "        # rgb = rgb / 0.2  # Vitus: / 0.4  # Samantha: / 0.22\n",
    "        # rgb = torch.clip(rgb, 0.0, 1.0)\n",
    "        # rgb = torch.nan_to_num(rgb, nan=0)\n",
    "\n",
    "        # l_boundaries = find_boundaries(l_labels, mode='outer').astype(float)  # T x H x W\n",
    "        # h_boundaries = find_boundaries(h_labels, mode='outer').astype(float)  # T x H x W\n",
    "\n",
    "        #\n",
    "        # PLOT\n",
    "        #\n",
    "        ndvi_lower_plot = double_logistic_function(t_plot.repeat(paramsl.shape[0], 1), paramsl.cpu())\n",
    "        ndvi_upper_plot = double_logistic_function(t_plot.repeat(paramsu.shape[0], 1), paramsu.cpu())\n",
    "\n",
    "        masked_ndvi = ndvi[:, forest_mask]\n",
    "        masked_ndvi = masked_ndvi.permute(1, 0)  # B x 7*73\n",
    "        masked_ndvi_test = masked_ndvi[:, SPLIT_IDX:].cpu()\n",
    "        masked_nan_mask_test = torch.isnan(masked_ndvi_test)\n",
    "        \n",
    "\n",
    "        fig, ax = plt.subplots(6, 2, figsize=(15, 18))\n",
    "        random_indices = np.random.choice(np.arange(paramsl.shape[0]), size=12, replace=False)\n",
    "        for pl_idx, bi in enumerate(random_indices):\n",
    "\n",
    "            row, col = divmod(pl_idx, 2)\n",
    "\n",
    "            iqr = ndvi_upper_plot[bi] - ndvi_lower_plot[bi]\n",
    "\n",
    "            masked_ndvi_test_i = masked_ndvi_test[bi][~masked_nan_mask_test[bi]]\n",
    "            masked_doy_test_i = doy_test[~masked_nan_mask_test[bi]].cpu()\n",
    "            ax[row, col].scatter(masked_doy_test_i, masked_ndvi_test_i, label='Observed NDVI')\n",
    "            ax[row, col].fill_between(t_plot[0] / T_SCALE, ndvi_lower_plot[bi], ndvi_upper_plot[bi], alpha=0.2, color='red')\n",
    "            ax[row, col].plot(t_plot[0] / T_SCALE, ndvi_lower_plot[bi] - 1.5 * iqr, c='k')\n",
    "            ax[row, col].plot(t_plot[0] / T_SCALE, ndvi_upper_plot[bi] + 1.5 * iqr, c='k')\n",
    "            ax[row, col].set_ylabel(\"NDVI\")\n",
    "            ax[row, col].set_ylabel(\"DOY\")\n",
    "            ax[row, col].set_ylim(-0.1, 1)\n",
    "\n",
    "        fig.show()\n",
    "\n",
    "        break\n",
    "\n",
    "# browse_images(rgb, ndvi, l_boundaries, h_boundaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outcome of the above\n",
    "l_anom_nn_np = l_anom_nn.cpu().numpy()\n",
    "h_anom_nn_np = h_anom_nn.cpu().numpy()\n",
    "\n",
    "l_anom_np = l_anom.cpu().numpy()\n",
    "h_anom_np = h_anom.cpu().numpy()\n",
    "\n",
    "interactive_map = map(minicube)\n",
    "\n",
    "bounds = [[float(minicube.lat[-1].values), float(minicube.lon[0].values)], [float(minicube.lat[0].values), float(minicube.lon[-1].values)]]\n",
    "\n",
    "colors = [(0, 0, 0, 0), (1, 0, 0, 1), (0, 1, 0, 1)]   # transparent, R, G\n",
    "cmap = mcolors.ListedColormap(colors)\n",
    "norm = mcolors.BoundaryNorm([-0.5, 0.5, 1.5, 2.5], cmap.N)\n",
    "\n",
    "colors_nn = [(0, 0, 0, 0), (1, 0.5, 0, 1), (0, 0, 1, 1)]   # transparent, Y, B\n",
    "cmap_nn = mcolors.ListedColormap(colors_nn)\n",
    "norm_nn = mcolors.BoundaryNorm([-0.5, 0.5, 1.5, 2.5], cmap.N)\n",
    "\n",
    "# define a month as anomalous if >80% of valid pixels in that month are outliers\n",
    "\n",
    "# cond_prob_months = np.array_split(cond_prob, 12, axis=0)\n",
    "l_anomal_nn = np.array_split(l_anom_nn_np, 12, axis=0)\n",
    "h_anomal_nn = np.array_split(h_anom_nn_np, 12, axis=0)\n",
    "\n",
    "l_anomal = np.array_split(l_anom_np, 12, axis=0)\n",
    "h_anomal = np.array_split(h_anom_np, 12, axis=0)\n",
    "\n",
    "# anom_threshold = -5\n",
    "for month_idx, (l_month, h_month, l_month_nn, h_month_nn) in enumerate(zip(l_anomal, h_anomal, l_anomal_nn, h_anomal_nn)):\n",
    "    # print(month_cp.shape)\n",
    "    # valid = ~np.isnan(month_cp)\n",
    "    # anom = np.zeros_like(month_cp)\n",
    "    # anom[valid & (np.log(month_cp) < anom_threshold)] = 1\n",
    "    # anom = np.sum(anom, axis=0) / np.sum(valid, axis=0) > 0.5\n",
    "\n",
    "    month = np.zeros((l_month.shape[1], l_month.shape[2]))\n",
    "    month[np.sum(l_month.astype(int), axis=0) > 0] = 1\n",
    "    month[np.sum(h_month.astype(int), axis=0) > 0] = 2\n",
    "\n",
    "    month_nn = np.zeros((l_month_nn.shape[1], l_month_nn.shape[2]))\n",
    "    # # if more than 80% are outliers\n",
    "    # l_month_nn[l_month_nn > 0] = 1\n",
    "    # h_month_nn[h_month_nn > 0] = 1\n",
    "    month_nn[np.nanmean(l_month_nn, axis=0) > 0.8] = 1\n",
    "    month_nn[np.nanmean(h_month_nn, axis=0) > 0.8] = 2\n",
    "    colored_anom = cmap(norm(month))\n",
    "    colored_anom_nn = cmap_nn(norm_nn(month_nn))\n",
    "    # alpha = np.where(month != 0, 0.2, 0)[:, :, None]\n",
    "    # rgb_anom = np.concatenate((colored_anom[..., :3], alpha), axis=2)\n",
    "    # rgb_anom = colored_anom\n",
    "\n",
    "    folium.raster_layers.ImageOverlay(\n",
    "        image=colored_anom,\n",
    "        bounds=bounds,\n",
    "        mercator_project=True,\n",
    "        name=f\"Statistical anomalies in month {month_idx})\",\n",
    "        opacity=0.8,\n",
    "    ).add_to(interactive_map)\n",
    "\n",
    "    folium.raster_layers.ImageOverlay(\n",
    "        image=colored_anom_nn,\n",
    "        bounds=bounds,\n",
    "        mercator_project=True,\n",
    "        name=f\"NN anomalies in month {month_idx})\",\n",
    "        opacity=0.8,\n",
    "    ).add_to(interactive_map)\n",
    "\n",
    "# for year in range(changes_ndvi.shape[1]):\n",
    "#     colored_ndvi = cmap(norm(changes_ndvi[:, year, :, :].squeeze()))\n",
    "\n",
    "#     alpha_ndvi = np.where((changes_ndvi[:, year, :, :].squeeze() != 0) & (cube.FOREST_MASK.squeeze() != 0), 1, 0)\n",
    "\n",
    "#     rgba_ndvi = np.dstack((colored_ndvi[..., :3], alpha_ndvi))\n",
    "\n",
    "#     folium.raster_layers.ImageOverlay(\n",
    "#         image=rgba_ndvi,\n",
    "#         bounds=bounds,\n",
    "#         mercator_project=True,\n",
    "#         name=f\"NDVI change (year {year})\",\n",
    "#         opacity=0.9,\n",
    "#     ).add_to(interactive_map)\n",
    "\n",
    "folium.map.LayerControl(position='topright', collapsed=False, autoZIndex=True).add_to(interactive_map)\n",
    "\n",
    "interactive_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmean(np.array([np.nan, np.nan])) > 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dimpeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
