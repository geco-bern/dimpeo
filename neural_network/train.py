import numpy as np
import os
import argparse
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
from torch.utils.tensorboard import SummaryWriter
from torch.utils.data import DataLoader, RandomSampler
from sklearn.metrics import d2_pinball_score
from tqdm import tqdm
from datetime import datetime

from neural_network.mlp import MLPWithEmbeddings
from neural_network.dataset import ChunkedZarrDataset, MEANS, STDS

def double_logistic_function(t, params):
    sos, mat_minus_sos, sen, eos_minus_sen, M, m = torch.split(params, 1, dim=1)
    mat_minus_sos = nn.functional.softplus(mat_minus_sos)
    eos_minus_sen = nn.functional.softplus(eos_minus_sen)
    sigmoid_sos_mat = nn.functional.sigmoid(
        -2 * (2 * sos + mat_minus_sos - 2 * t) / (mat_minus_sos + 1e-10)
    )
    sigmoid_sen_eos = nn.functional.sigmoid(
        -2 * (2 * sen + eos_minus_sen - 2 * t) / (eos_minus_sen + 1e-10)
    )
    return (M - m) * (sigmoid_sos_mat - sigmoid_sen_eos) + m


def objective_pinball(params, t, ndvi, nan_mask, alpha=0.5, weights=None):
    ndvi_pred = double_logistic_function(t, params)
    diff = ndvi - ndvi_pred
    loss = torch.max(torch.mul(alpha, diff), torch.mul((alpha - 1), diff))
    # we need to reweight the quantiles to prevent a degenerate solution
    if weights is not None:
        loss = loss * weights.unsqueeze(0)
    return torch.mean(loss[~nan_mask])


def train(name, data_path, features=None, model_type='logistic'):
    torch.manual_seed(1)

    run_id = f"{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    writer = SummaryWriter(log_dir=f"/data_2/scratch/sbiegel/processed/runs/{name}/{model_type}/{run_id}")
    checkpoint_dir = f"/data_2/scratch/sbiegel/processed/checkpoints/{name}/{model_type}/{run_id}"
    os.makedirs(checkpoint_dir, exist_ok=True)
    print(f"Writing logs to {checkpoint_dir}")
    print(f"Run directory: {writer.log_dir}")

    num_epochs = 20
    lr = 0.005
    lr_decay_rate = 0.01
    batch_size = 1024
    device = "cuda"

    print("Starting model with:")
    print("name = {}".format(name))

    print("Loading dataset...")
    if isinstance(features, str):
        features = features.split(",")
    print("Using features: {}".format(ds.features))

    ds = ChunkedZarrDataset(
        data_path,
        features,
        batch_size=batch_size,
        chunk_size=8192,
        shuffle_chunks=True,
        seed=42,
    )
    missingness = ds.missingness
    missingness = torch.from_numpy(missingness).to(device)
    t = torch.from_numpy(ds.t).float().to(device)
    
    loader = DataLoader(
        ds,
        batch_size=None,
        num_workers=4,
        pin_memory=True,
        prefetch_factor=2,
        persistent_workers=True,
    )

    means_pt = torch.tensor([MEANS[f] for f in ds.num_features]).unsqueeze(0)
    stds_pt = torch.tensor([STDS[f] for f in ds.num_features]).unsqueeze(0)

    nr_num_features = ds.nr_num_features
    nr_species = ds.nr_tree_species
    nr_habitats = ds.nr_habitats

    d_out=18

    # this model has ~475k parameters
    encoder = MLPWithEmbeddings(
        d_num=nr_num_features,
        d_out=d_out,
        n_blocks=8,
        d_block=256,
        dropout=0.0,
        skip_connection=True,
        n_species=nr_species,
        species_emb_dim=4,
        n_habitats=nr_habitats,
        habitat_emb_dim=8,
    ).to(device)

    optimizer = torch.optim.AdamW(encoder.parameters(), lr=lr, weight_decay=1e-4)

    print(
        "Number of parameters: {}".format(
            sum(p.numel() for p in encoder.parameters() if p.requires_grad)
        )
    )

    NC_GRID_SIZE = 32
    t_grid = torch.linspace(0, 1.0, NC_GRID_SIZE, device=device).unsqueeze(0)

    print("Starting training...")

    n_iterations = 0
    total_iterations = num_epochs * ds.n_batches
    for epoch in range(num_epochs):
        ds.set_epoch(epoch)
        print(f"Starting epoch {epoch + 1}")
        for sample in tqdm(loader, total=len(loader)):
            ndvi, feat = sample

            nan_mask = torch.isnan(ndvi) | (ndvi == -2**15) | (ndvi == 2**15 - 1)
            ndvi = ndvi.float() / 10000.0
            outlier_mask = (ndvi > 1) | (ndvi < -0.1)
            nan_mask = nan_mask | outlier_mask

            feat_num = feat[:, ds.num_feature_indices]
            feat_species = feat[:, ds.mapping_features["tree_species"]].int()
            feat_habitat = feat[:, ds.mapping_features["habitat"]].int()
            feat_species[feat_species == 255] = 16

            # standardize input
            feat_num = (feat_num - means_pt) / stds_pt

            feat_num = feat_num.to(device, non_blocking=True)
            feat_species = feat_species.to(device, non_blocking=True)
            feat_habitat = feat_habitat.to(device, non_blocking=True)

            t_ndvi_train = ndvi.float().to(device, non_blocking=True)
            t_nan_mask_train = nan_mask.to(device, non_blocking=True)

            preds = encoder(
                feat_num,
                feat_species,
                feat_habitat,
            )

            paramsl = preds[:, [0, 1, 2, 3, 4, 5]]
            paramsm = preds[:, [6, 7, 8, 9, 10, 11]]
            paramsu = preds[:, [12, 13, 14, 15, 16, 17]]

            lossl = objective_pinball(
                paramsl,
                t,
                t_ndvi_train,
                t_nan_mask_train,
                alpha=0.25,
                weights=missingness,
            )
            lossm = objective_pinball(
                paramsm,
                t,
                t_ndvi_train,
                t_nan_mask_train,
                alpha=0.50,
                weights=missingness,
            )
            lossu = objective_pinball(
                paramsu,
                t,
                t_ndvi_train,
                t_nan_mask_train,
                alpha=0.75,
                weights=missingness,
            )

            # Add constraint to ensure periodicity
            t_start = torch.full((feat.shape[0], 1), 0, device=device)
            t_end   = torch.full((feat.shape[0], 1), 1, device=device)
            startl = double_logistic_function(t_start, paramsl)
            endl = double_logistic_function(t_end, paramsl)
            periodic_loss_l = torch.mean((startl - endl) ** 2)
            startm = double_logistic_function(t_start, paramsm)
            endm = double_logistic_function(t_end, paramsm)
            periodic_loss_m = torch.mean((startm - endm) ** 2)
            startu = double_logistic_function(t_start, paramsu)
            endu = double_logistic_function(t_end, paramsu)
            periodic_loss_u = torch.mean((startu - endu) ** 2)
            total_periodic_loss = periodic_loss_l + periodic_loss_m + periodic_loss_u

            lambda_periodic = 1

            t_grid_b = t_grid.repeat(paramsl.shape[0], 1)

            ndvi_lower_grid = double_logistic_function(t_grid_b, paramsl)
            ndvi_middle_grid = double_logistic_function(t_grid_b, paramsm)
            ndvi_upper_grid = double_logistic_function(t_grid_b, paramsu)
            violation_lu = torch.relu(ndvi_lower_grid - ndvi_upper_grid)
            violation_lm = torch.relu(ndvi_lower_grid - ndvi_middle_grid)
            violation_mu = torch.relu(ndvi_middle_grid - ndvi_upper_grid)
            violation = violation_lu + violation_lm + violation_mu
            per_sample_noncross = violation.mean(dim=1)
            total_noncross = per_sample_noncross.mean()

            lambda_nc = 10.0

            loss = lossl + lossm + lossu + lambda_periodic * total_periodic_loss + lambda_nc * total_noncross

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            new_lrate = lr * (lr_decay_rate ** (n_iterations / total_iterations))
            for param_group in optimizer.param_groups:
                param_group["lr"] = new_lrate

            if (n_iterations + 1) % 10 == 0:
                writer.add_scalar("Loss/train", loss, n_iterations+1)
                for pi, param_group in enumerate(optimizer.param_groups):
                    writer.add_scalar(
                        "LearningRate[{}]".format(pi), param_group["lr"], n_iterations
                    )

            if (n_iterations + 1) % 100 == 0:
                with torch.no_grad():
                    fig, ax = plt.subplots(2, 2, figsize=(15, 6), sharey=True)

                    t_fit = (
                        torch.linspace(0, 1, 1000)
                        .unsqueeze(0)
                        .repeat(paramsl.shape[0], 1)
                    )
                    ndvi_lower = double_logistic_function(
                        t_fit, paramsl.cpu()
                    )
                    ndvi_middle = double_logistic_function(
                        t_fit, paramsm.cpu()
                    )
                    ndvi_upper = double_logistic_function(
                        t_fit, paramsu.cpu()
                    )

                    random_indices = np.random.choice(
                        np.arange(paramsl.shape[0]), size=4, replace=False
                    )
                    for pl_idx, bi in enumerate(random_indices):
                        row, col = divmod(pl_idx, 2)

                        masked_ndvi_train = ndvi[bi][~nan_mask[bi]]

                        # Convert fractional year (t) back to day-of-year for plotting
                        doy_array = (ds.t * 365).astype(np.float32)
                        masked_doy_train = doy_array[~nan_mask[bi]]

                        ax[row, col].scatter(masked_doy_train, masked_ndvi_train, label="Observed NDVI")
                        ax[row, col].fill_between(
                            (t_fit * 365)[bi],
                            ndvi_lower[bi],
                            ndvi_middle[bi],
                            alpha=0.2,
                            color="red",
                        )

                        ax[row, col].fill_between(
                            (t_fit * 365)[bi],
                            ndvi_middle[bi],
                            ndvi_upper[bi],
                            alpha=0.2,
                            color="green",
                        )
                        ax[row, col].set_xlim(0, 365)
                        ax[row, col].set_xlabel("Day of year")
                        ax[0, 0].set_ylabel("NDVI")
                        ax[1, 0].set_ylabel("NDVI")

                    writer.add_figure(f"Fit/iter_{n_iterations+1}", fig, n_iterations)

                    ndvi_lower = double_logistic_function(t.cpu(), paramsl.cpu())
                    ndvi_middle = double_logistic_function(t.cpu(), paramsm.cpu())
                    ndvi_upper = double_logistic_function(t.cpu(), paramsu.cpu())

                    all_masked_ndvi_lower = ndvi_lower[~nan_mask].cpu()
                    all_masked_ndvi_middle = ndvi_middle[~nan_mask].cpu()
                    all_masked_ndvi_upper = ndvi_upper[~nan_mask].cpu()
                    all_masked_ndvi_train = ndvi[~nan_mask].cpu()
                    d2_score_lower = d2_pinball_score(
                        all_masked_ndvi_train, all_masked_ndvi_lower, alpha=0.25
                    )
                    d2_score_middle = d2_pinball_score(
                        all_masked_ndvi_train, all_masked_ndvi_middle, alpha=0.50
                    )
                    d2_score_upper = d2_pinball_score(
                        all_masked_ndvi_train, all_masked_ndvi_upper, alpha=0.75
                    )

                    writer.add_scalar(
                        "D2PinballScoreLower/train", d2_score_lower, n_iterations
                    )
                    writer.add_scalar(
                        "D2PinballScoreMiddle/train", d2_score_middle, n_iterations
                    )
                    writer.add_scalar(
                        "D2PinballScoreUpper/train", d2_score_upper, n_iterations
                    )
            n_iterations += 1

        torch.save(encoder.state_dict(), f"{checkpoint_dir}/encoder_epoch{epoch+1}.pt")

    torch.save(
        encoder.state_dict(),
        f"{checkpoint_dir}/encoder.pt"
    )

    writer.flush()
    writer.close()


if __name__ == "__main__":
    parser = argparse.ArgumentParser("DIMPEO Training")
    parser.add_argument("-n", "--name", type=str, default="dimpeo_training")
    parser.add_argument(
        "--data-path",
        type=str,
        default="/data_2/scratch/sbiegel/processed/ndvi_dataset_filtered_shuffled.zarr",
    )
    parser.add_argument(
        "--features",
        type=str,
        default="dem,slope,easting,northing,twi,tri,mean_curv,profile_curv,plan_curv,roughness,median_forest_height,forest_mix_rate,tree_species,habitat",
    )
    parser.add_argument(
        "--model_type",
        type=str,
        default="logistic"
    )
    args = parser.parse_args()

    train(args.name, args.data_path, args.features, args.model_type)
